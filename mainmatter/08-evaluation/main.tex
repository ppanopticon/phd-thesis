\chapter{Evaluation}
\label{chapter:evaluation}

In this chapter we present the evaluation of \cottontail{} \cite{Gasser:2020Cottontail}, which implements the previously described concepts and has become an integral part of the \vitrivr{} \cite{Rossetto:2016vitrivr,Gasser:2019Towards,Gasser:2019Multimodal} stack. For this quantiative evaluation, we focus on three aspects:

\begin{description}
    \item[Interactive Multimedia Retrieval Workloads] These benchmarks focus on interactive video retrieval workloads on million-scale datasets, for which \cottontail{} was primarily designed. We compare the efficiency and effectiveness of different access methods (scan, index) and execution models (multi-threading, \acrshort{simd}). For this evaluation, we use the Vimeo Creative Commons Collection 1 \& 2 (V3C) \cite{Berns:2019V3C1,Rossetto:2021Insights}, which currently serves as the reference dataset for both \acrshort{vbs} and TRECVID AVS. The dataset consists of 17235 videos, which amounts to \SI{2300}{\hour} of content. We use a series of features derived by \vitrivr{}'s feature extraction engine Cineast as listed in \Cref{table:datasets}.
    \item[Large-Scale Similarity Search] In these tests, we examine how \cottontail{} scales out to billion-scale datasets as employed in challenges such as the (Big-) ANN-Benchmark \cite{Aumueller:2017ANN,Simhadri:2022Results}. While not the primary focus of \cottontail{}'s design, this is still an interesting task to obtain a baseline for future efforts. We use this opportunity to compare \cottontail{} to Milvus \cite{Wang:2021Milvus} in a series of large-scale \acrshort{nns} queries. All queries are run against shards of the Yandex Deep1B dataset \cite{Babenko:2016Efficient}, which is a collection of descriptors derived from a GoogLeNet \cite{Szegedy:2015Going} neural network.
    \item[Adaptive Index Management] This test examines \cottontail{} ability to cope with changes to datasets and associated index structures that take place concurrently. We demonstrate the deterioration of indexing quality and the automated mechanisms that maintains up-to-date indexes.
\end{description}

\section{Evaluation Setup}

All evaluations are centered around database queries that are sent to the system under testing. We generate measurements from these queries and the results they return. Typically, we average the obtained values over $10$ repetitions in order to compensate for anomalies and we execute a single warm-up query to give the system's the ability to initialise necessary caches and other optimisations 

Rather than working with randomly generated data, we decided to use existing data corpora that reflect the structure of features as generated by different, real-world models. Those data sets are listed in \Cref{table:datasets} and we will refer to the collections and entities by their name throughout this chapter. For the the Yandex Deep1B \cite{Babenko:2016Efficient} dataset we have prepared different shards that contain the first 5 million, 10 million, 100 million and 1 billion vectors.

Data is imported as a dedicated entity (\cottontail{}) or collection (Milvus) respectively, prior to executing the actual workloads. Index structures are also prepared beforehand if not stated otherwise. Every entity has the columns \texttt{id} and \texttt{feature}. The \texttt{id} column serves as a primary key and is either a \texttt{string} (all V3C-based datasets) or a \texttt{long} value (all Deep1B-based datasets). The Deep1B shards also exhibit an additional \texttt{category} column, which holds a randomly assigned \texttt{int} value that is used for Boolean filtering. The Deep1B datasets also come with query vectors and groundtruth data, which we use accordingly.

\begin{table}
    \begin{tabular}{ | l | c | c | c | p{5cm} |}
        \hline
        \textbf{Entity} & \textbf{Source} & \textbf{d} & \textbf{N} & \textbf{Description} \\
        \hline
        \hline
        features\_averagecolor & V3C1 \& 2  & 3 & 2512715 & Average colours derived from video segments. \\ 
        \hline
        features\_visualtextcoembedding & V3C1 \& 2 & 25 & 2506273 & Video to text co-embedding derived from vide segments. See \cite{Spiess:2021Competitive}. \\
        \hline
        features\_surfmf25k512  & V3C1 \& 2  & 512 & 2500943 & SURF \cite{Bay:2006surf} describtors derived from video segments. \\
        \hline
        features\_inceptionresnetv2 & V3C1 \& 2  & 1536 & 2508358 & Vector derived from last fully connected layer of a pre-trained InceptionResNetV2 applied on video segments.\\
        \hline
        features\_conceptmasksade20k & V3C1 \& 2 & 2048 & 2469844 & Image embedding for query by semantic sketch. See \cite{Rossetto:2019Query}. \\
        \hline
        yandex\_deep5M  & Deep1B  & 96 & $5000000$ & See \cite{Babenko:2016Efficient} for more details. \\
        \hline
        yandex\_deep10M  & Deep1B & 96 & $10000000$ & See \cite{Babenko:2016Efficient} for more details. \\
        \hline
        yandex\_deep100M  & Deep1B & 96 & $100000000$ & See \cite{Babenko:2016Efficient} for more details. \\
        \hline
        yandex\_deep1B  & Deep1B & 96 & $1000000000$ & See \cite{Babenko:2016Efficient} for more details. \\
        \hline
        \hline
    \end{tabular}
    \caption{The data collections used for this evaluation. }
    \label{table:datasets}
\end{table}

\cottontail{} and Milvus are deployed on the same physical server but they do not run concurrently. This server exhibits two \acrshort{numa} nodes with an Intel Xeon CPU E5-2630 v4 (20 cores@\SI{2.2}{\giga\hertz}) and \SI{192}{\giga\byte} \acrshort{ram} each. Therefore, the machine provides 40 compute cores and a total of \SI{384}{\giga\byte} of \acrshort{ram}. The \acrshort{cpu} supports the AVX2 instruction set extension, which allows for vectorised execution. The data directories from which Milvus and \cottontail{} access their data resides on three \SI{500}{\giga\byte} \acrshort{ssd}'s that have been combined into a single, logical disk using \acrshort{raid}0 (striping). The server runs Ubuntu 20.04 and the OpenJDK Java version 17.0.3. All queries are sent from a separate computer on the same network to minimise the impact on query execution.


\subsection{Metrics}

We assume that for every query, we receive a list of results $R$ from the respective database. That list contains $K$ items $r_i \in R, i \in \lbrack 1, K  \rbrack $ that match the query. The returned items $r_i$ are ranked either explicitly based on score or distance (for \acrshort{nns}, \acrshort{fns} or range search) or just arbitrarily, i.e., $r_1$ comes at position one, $r_2$ at position two and so forth. The ranking is solely dependent on the database and query. Every item $r_i$ is simply a tuple containing the primary key of the retrieved database entry, which we always use to test for equality.

For every query, we are interested in assessing the efficiency and effectiveness of its execution. Efficiency can be easily gauged in terms of query execution time, which is the elapsed real time in seconds between issuing the query and receiving the results. This metric is simple enough to obtain and reason about and does not require further elaboration. The effectiveness of the query, i.e., the quality of the results it produces, is a bit more complicated and therefore assessed by two separate metrics: Given a result $R$ produced by the database and a known groundtruth $G$, i.e., a list of results that we know to be correct, we obtain both the recall as well as the \acrfull{dcg} \cite{Jarvelin:2002Cumulated} of the result compared to the groundtruth at a given level $k \leq K$. Henceforth, we will use $k$ as subscript to indicate, that a fixed number if items is considered.

The reason for obtaining two metrics for result quality lies in the limitations of the metrics themselves. Recall at a fixed level $k$, for which a definition is provided in \Cref{equation:recall}, is purely set-based and simply checks for the existence of an element from the groundtruth $G_k$ in the result $R_k$, without taking the exact positioning of the items into account.

\begin{equation}
    \label{equation:recall}
    \texttt{REC}_k (R_k, G_k) = \frac{|R_k \cap G_k |}{k}
\end{equation}

If $R_k$ contains items that are not contained in the groundtruth (false positives) or $G_k$ contains documents that are missing in $R_k$ (false negative), this directly results in a drop in $\texttt{REC}_k$. Therefore, if all of the items $r_i \in R_k$ were to be contained in $G_k$, i.e., $R_k \cap G_k = G_k$, the recall would become $1.0$ and a result could be considered a perfect match. In contrast, if none of the items in $r_i \in R_k$ were contained in $G_k$, i.e., $R_k \cap G_k = \emptyset$ then recall drops to $0.0$. However, recall does not provide us with any information about the ranking of the individual items. In an extreme case, two lists containing the same items but in a reversed order would produce the same recall value of $1.0$. Often, though, we find that items with a low rank are more important than items with a very high rank. This is true both when considering a human user browsing a list from top to bottom, typically paying attention only to the top entries, or a use-case in which we are only interested in the top item(s) (see, for example, \Cref{section:application_mrf}). Nevertheless, and despite these limitations, the recall metric and variants thereof are very popular in \acrshort{anns} evaluations \cite{Aumueller:2017ANN,Simhadri:2022Results}. 

To compensate for the absence of information about the ranking quality, we turn to a variant of the \acrshort{dcg} at a given level $k$, which in its original form was proposed by \cite{Jarvelin:2002Cumulated}. Our adapted version is specified in \Cref{equation:dcg}. It builds the sum over all items $r_i \in R_k$. The numerator of the expression specifies the assigned relevance of an item based on its position in the groundtruth $G_k$, which is expressed by the function $\text{rel} (r_i)$. If $r_i$ has rank $1$ in $G_k$, it is considered most relevant and thus receives the relevance $k$. If $r_i$ has rank $k$ in $G_k$, it is not considered very releveant and therefore receives the relevance $1$. If $r_i$ is not contained in $G_k$ at all (false positive), then $\text{rank} (g_i)$ returns the lowest possible gain $0$. The denominator is the logarithm of the actual rank of $r_i \in R_k$. 

\begin{eqnarray}
\label{equation:dcg}
\mathtt{DCG}_k (R_k, G_k)= \sum_{i = 1}^{k} \frac{\text{rel}(r_i) + 1}{\log_2(i + 1)} \\
\text{rel} (r_i) = 
    \begin{cases}
        k - \text{rank}_{G_k}(r_i), &  \text{if } r_i \in G_k \\
        0,                          &  \text{if } r_i \notin G_k
    \end{cases}
\end{eqnarray}

The reasoning behind using this \acrshort{dcg} variant is that items with a low rank in $G_k$ are more important than items with a very high rank. The numerator accounts for this by assigning high relevance to items that appear early on, which quantifies the gain of inspecting such an item. That gain is discounted by an item's actual rank in $R_k$, i.e., the further down in the list an item appears the larger the discount. Obviously, if an item in the result $R_k$ does not appear in the groundtruth (false positive), then there is no gain at all. What is not directly captured by the \acrshort{dcg}, however, are entries that appear in $G_k$ but that are missing in $R_k$ (false negatives). However, this is again covered by the recall. To make the \acrshort{dcg} values comparable across queries (potentiall with different values of $k$), we normalise it by the ideal $\mathtt{iDCG}_K$ to obtain the normalised $\mathtt{nDCG}_K$ as indicated by \Cref{equation:ndcg}. The $\mathtt{iDCG}_K$ simply quantifies the maximum perfect ranking given groundtruth $G_K$. Therefore, the normalised $\mathtt{nDCG}_K$ always assumes values between $0.0$ and $1.0$. All the metrics described here are also illustrated in \Cref{example:result_and_metrics}.

\begin{align}
    \label{equation:ndcg}
    \mathtt{nDCG}_k(R_, G_k) &= \frac{\mathtt{DCG}_k(R_K, G_K)}{\mathtt{iIDCG}_K(G_K)} \\
    \mathtt{iIDCG}_k(G_k) &= \sum_{i = 0}^{K} \frac{k + 1 - i}{\log_2(i + 1)}
\end{align}

\begin{example}[label=example:result_and_metrics]{Result $R$, Groundtruth $G$ and obtained metrics.}{}
    We consider the following result $R$ and the associated groundtruth $GT$ ($k = 7$), which contain the same items but in exact reverse order.

    \begin{center}
        \begin{tabular}{ l || l | l || l | l | l | l |}
            \textbf{Rank} & \textbf{GT} & $ k + 1 - i$ & \textbf{R} & $R_k \cap G_k$ &  $\text{rel}(r_i)$ & $\log_2(i + 1)$ \\ 
            \hline
            \hline
            $1$ & 87  & 7 & 597  & \cmark & 2 & 2.58 \\
            \hline
            $2$ & 123  & 6 & 331 & \cmark &  3 & 2.32 \\
            \hline
            $3$ & 542 & 5 & 3213 & \cmark & 4 & 2 \\
            \hline
            $4$ & 3213 & 4 & 542 & \cmark &  5 & 1.58 \\
            \hline
            $5$ & 313 & 3 & 123 & \cmark &  6 & 1 \\
            \hline
            $6$ & 597 & 2 & 87 & \cmark &  7 & 1 \\
            \hline
            $7$ & 757 & 1 & 888 & \xmark & 0 & 1 \\
            \hline
        \end{tabular}
    \end{center}


    The values for $\mathtt{REC}_7$, $\mathtt{DCG}_7$, $\mathtt{IDCG}_7$ and $\mathtt{NDCG}_7$ according to Equations \ref{equation:recall}, \ref{equation:dcg} and \ref{equation:ndcg} are given as follows.

    \begin{align*}
        \label{equation:dcg_example}
        \mathtt{REC}_7 &= \frac{6}{7} = 0.86 \\
        \mathtt{DCG}_7 &= \frac{2}{1} + \frac{3}{1.58} + \frac{4}{2} + \frac{5}{2.32} + \frac{6}{2.58} + \frac{7}{2.81} + \frac{0}{3} = 12.87 \\
        \mathtt{IDCG}_7 &= \frac{7}{1} + \frac{6}{1.58} + \frac{5}{2} + \frac{4}{2.32} + \frac{3}{2.58} + \frac{2}{2.81} + \frac{1}{3} = 17.23 \\
        \mathtt{NDCG}_7 &= \frac{12.87}{17.23} = 0.75
    \end{align*}

\end{example}



\section{Interactive Multimedia Retrieval}
\todo[inline]{vitrivr's participation to VBS, LSC etc, DRES.}


\section{Large-Scale Similarity Search}
This series of measurements is a direct comparison between Milvus and \cottontail{}. We execute both \acrshort{nns} and hybrid queries on prepared shards of the Deep1B \cite{Babenko:2016Efficient} dataset using different settings and compare the obtained metrics.  The shards contain 5 million, 10 million, 100 million and 1 billion $96$-dimensional \texttt{float} vectors and are stored in dedicated collections (Milvus) or entities (\cottontail). The Psudo-\acrshort{sql} of the queries is listed in Listings \Cref{listing:big_nns_query}. We use the query vectors provided with the Yandex Deep1B dataset and establish a groundtruth for each query by executing a brute-force search without the use of an index. The

\begin{lstlisting}[language=SQL, caption={Pseudo-SQL of the queries executed for this measurement.}, label=listing:big_nns_query, numbers=none]
    /* Simple NNS. */
    select id, euclidean(feature, <query>) as dst from <collection> order by dst limit 1000

    /* Hybrid query. */
    select id, euclidean(feature, <query>) as dst from <collection> where category = <category> order by dst limit 1000
    
    /* NSS that returns feature vectors. */
    select id, feature, euclidean(feature, <query>) as dst from <collection> order by dst limit 1000
\end{lstlisting}

For every type of query, we also visualise the execution plan generated by \cottontail{} for reference.

\subsection{\cottontail}

\subsection{Milvus}
We have already introduced Milvus \cite{Wang:2021Milvus} in \Cref{section:milvus} and we have selected it as a baseline for our billion-scale evaluation. The basic setup is identical to that used for \cottontail{}, i.e., Milvus runs on a single node, despite its ability to scale out to large clusters, in order to keep the setup comparable. We use the Milvus standalone version, which runs in a Docker container, and we followed the official tutorial for setting it up and using it \footnote{See https://milvus.io/docs/v2.0.x/, Accessed July 2022}. All benchmarks were conducted with the latest version 2.0.2 without further adjustments to the default configuration, which should not be necessary in this rather simple setup. In this default configuration, Milvus is allowed to use all available resources in terms of \acrshort{cpu}, \acrshort{ram} and disk space.

For executing queries, the mode of operation is a bit different than it is for \cottontail, since Milvus requires a data collection to be available in main memory, in order to be able to query it. Therefore, every query consists of two steps: Loading the collection and then executing the query once the collection is ready. We have obtained the ellapsed time for loading the data and query execution separately, so that we can reason about the individual components. The way these queries can be specified is outlined in \Cref{listing:milvus_query}. We show the Python instead of the Java syntax, because it is less verbose and more readable, assuming, that the functionality of the two client libraries is identical.  

\begin{lstlisting}[language=Python, caption={Example of a similarity search query to Milvus in Python on a 2-dimensional vector with the prior loading of a data collection.}, label=listing:milvus_query, numbers=none]
    from pymilvus import Collection
    
    # Load an existing collection.
    collection = Collection("images")      
    collection.load()

    # Perform search.
    query = [[0.0, 0.0]]
    params = {"metric_type": "L2", "params": {"nprobe": 10}}
    results = collection.search(
        data=query, anns_field="feature", param=params, limit=10
    )
\end{lstlisting}

In our first series of test we use the default \texttt{FLAT} index, which is the equivalent of an exhaustive search without compression or quantisation. This is the only index able to guarantee a recall of $1.0$. The results of this series of tests is depicted in Figures \ref{figure:milvus_flat_runtime} and \ref{figure:milvus_flat_quality}. We can summarise that even for brute-force search, query execution speed is very impressive once a collection has been loaded into main memory. For our test collection, it took between $0.09 \pm 0.02 \, \si{\second}$ (5 million) and $2.5 \pm 1.64 \, \si{\second}$ (100 million) to execute a query, all the while retaining a perfect recall and \acrshort{dcg} of $1.0$. However, if one considers time to load a collection from disk, these numbers raise to between $12.51 \pm 0.32 \, \si{\second}$ and $172.03 \pm 2.57 \, \si{\second}$. If one loads a collection once to then execute many queries, this is acceptable because the cost of loading the collection is amortised over time. However, if a large number of collections must be queried without the ability to predict which one, and not all collections can be pre-loaded, this becomes problematic. Furthermore, the requirement to load the collection prior to querying them turned out to be an insurmountable roadblock for the shard that contained 1 billion entries. Milvus was unable to load the collection as it ran out of the \SI{376}{\giga\byte} of available memory. If we assume that a vector component occupies \SI{4}{\byte}, then this is not surprising, since the entire collection of $96$-dimensional vectors would use approximately \SI{384}{\giga\byte}, ignoring space for primary key column and necessary metadata.

Since the available memory is obviously a limiting factor, we conducted a second series of measurements using the \texttt{IVF\_SQ8} index structure, hoping, that it would alleviate the memory pressure. This index structure uses an inverted file of clusters and limits search to a subset of these clusters based on the parameters provided by the user and an initial distance calculation between the query and the cluster centers. The approach is comparable to the two stage quantisation process described in \cite{Jegou:2010Product}, where each vector is first mapped to an inverted list using a coarse quantizer and then quantizsed using a second product quantiser. In addition to limiting the search space, the \texttt{IVF\_SQ8} index also compresses the \SI{4}{\byte} (\texttt{float}) into a \SI{1}{\byte} (\texttt{int8}) presentation, which significantly reduces memory and \acrshort{cpu} usage by up to 70\%. 

We created the \texttt{IVF\_SQ8} with $\mathtt{nlist} = 1024$ clusters and executed the query telling Milvus to search all the buckets $\mathtt{nprobe} = 1024$. The results are shown in Figures \ref{figure:milvus_ivfsq8_1024_runtime} and \ref{figure:milvus_ivfsq8_1024_quality}. Unfortunately, the memory problems were not resolved by the  \texttt{IVF\_SQ8}, because apparently Milvus now loads both the index structure as well as the original data, leading to even more memory use. This is also signified by the higher combined data loading and query times of between $14.44 \si{\second}$ (5 million) and $214.72 \si{\second}$ (100 million). The use of the index seems to have little impact on either query execution performance or quality, which is not surprsing since the improvement would come from pruning the search space, which we did not do in this case due to the choice of parameters. Again, though, we were unable to query the 1 billion entries shard of the dataset. 

To test for the impact of the \texttt{IVF\_SQ8} index, we changed the query parameter to limit ourselves to one fourth of the clusters, i.e., $\mathtt{nprobe} = 256$. The results are depicted in Figures \ref{figure:milvus_ivfsq8_256_runtime} and \ref{figure:milvus_ivfsq8_256_quality}.
\begin{landscape}
\begin{figure}[p]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/milvus-flat/runtime}
        \caption{Runtime, \texttt{FLAT} index}
        \label{figure:milvus_flat_runtime}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/milvus-ivfsq8-1024/runtime}
        \caption{Runtime, \texttt{IVF\_SQ8} index, $\mathtt{nprobe} = 1024$}
        \label{figure:milvus_ivfsq8_1024_runtime}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/milvus-flat/quality}
        \label{figure:milvus_flat_quality}
        \caption{Quality, \texttt{FLAT} index}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/milvus-ivfsq8-1024/quality}
        \caption{Quality, \texttt{IVF\_SQ8} index, $\mathtt{nprobe} = 1024$}
        \label{figure:milvus_ivfsq8_1024_quality}
    \end{subfigure}
    \caption{Metrics obtained with Milvus on Yandex Deep1B \cite{Babenko:2016Efficient} shards using different indexes and configurations.}
    \label{figure:milvus_noindex}
\end{figure}
\end{landscape}

\subsubsection{Qualitative Assesment}
While very convincing in terms of sheer execution speed, the current version of Milvus also has some limitations that we list here for future reference in addition to the requirement of pre-loading collections (non exhaustive):

\begin{itemize}
    \item Milvus currently only supports the Euclidean and Inner Product distance for nearest neighbour search on floating point embeddings.
    \item Milvus does not support proximity-based search strategies other than \acrshort{nns}. However, range search is due for the 2.2.0 release according to the official GitHub issue tracker.
    \item Milvus cannot retrieve the actual feature vectors as part of a proximity-based query, i.e., they must be fetched in an additional query by looking them up based on the primary key. This issue is also due for being addressed in the 2.2.0 release.
    \item Milvus can only maintain a single index per feature column, effectively limiting the set of available distance functions to one, since most indexes are trained for a specific index.
\end{itemize}

Furthermore, we have observed that as Milvus uses up all the memory during collection loading, it starts to behave rather eratic making it difficult to unload the collection again. In some of our runs, it even started to reload the same collection after a restart, effectively rendering the entire instance unusable for several hours.

\section{Adaptive Index Management}

\todo[inline]{Brute force vs. plain index vs. index with auxilary data structure}1