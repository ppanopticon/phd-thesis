\chapter{Multimedia Data Management}
\label{chapter:theory_multimedia_database}

The insight that special requirements must be addressed to accomodate multimedia data in traditional \acrshort{dbms} can be traced back to the 1990s \cite{Marcus:1996Foundations,Adjeroh:1997Multimedia} with many theoretical contributions but very little in terms of concrete (open-source) implementations. Instead, multimedia retrieval remained a field of research that remained largely seperated from that of database research with the exception of a few, isolated contributions that hinted in a direction of a unified approach \cite{Chengkai:2005RankSQL,Zhang:2006Boolean} as well as experimental systems that tried to address certain shortcomings of the relational model in the context of similarit search or scientific applications \cite{Silva:2010SimDB,Stonebraker:2013SciDB}.

If we go back and explore how early multimedia retrieval systems operated in the early days, we are likely to arrive at IBM's QBIC system \cite{Flickner:1995Query}, which allowed for (visual) similarity search on image and video data. QBIC's system architecture hints at the existence of a dedicated database layer used for storing features. However, there is little detail as to what type of database was used and what tasks were executed at a database level. Judging from the architecture and description, it seems that the queries were rather processed by the ``matching engine'' and the database itself was only used for storage. Furthermore, QBIC strictly distinguished between ``database population'' (offline) and ``database query'' (online) phases as have many systems thereafter.

Similarly, the MUVIS \cite{Kiranyaz:2003Muvis} system employed multiple databases for storing image, video and audio information separately without providing an explicit reason for this mode of operation. However, there is little detail as to what tasks fell to these database layers and which tasks were carried out by the ``MBrowser'' layer built on top of it. Interestingly, we can again observe the distinction between ``indexing'' (offline) and ``retrieval'' (online) for MUVIS as well. Unfortunately, both the QBIC and the MUVIS system are not publicly available.

A noteworthy attempt at providing a data management layer for content-based image retrieval was \acrfull{lire} \cite{Luc:2008LIRE}, which is an open source Java library based on the Apache Lucene \footnote{See https://lucene.apache.org/} engine used for fulltext search and retrieval. While \acrshort{lire} was a step in the right direction, it still tightly integrated feature extraction and storage and was thus of very limited use when trying to use different types of features.

\section{vitrivr, ADAM and \adampro{}}

To the best of our knowledge, the open source \vitrivr{} video retireval \cite{Rossetto:2016vitrivr} and later multimedia retrieval \cite{Gasser:2019Multimodal} system was one of the first systems to advertise a 3-tier architecture that included a dedicated multimedia database in addition to a feature extraction and fusion engine and a user interface. The first generation database layer came in the form of ADAM \cite{Giangreco:2014Adam}, which can be seen as an early attempt at bridging the gap between databases and multimedia retrieval. ADAM was basically an extension to PostgreSQL -- called \emph{dbADAM} -- that added support for \acrshort{nns} and \acrshort{vaf}-based indexes \cite{Weber:1998Va}. In that it is comparable to projects like \emph{pgvector} \footnote{See https://github.com/pgvector/pgvector/} and PASE \cite{Yang:2020Pase}, which however use more up-to-date index structures such as \acrshort{pq} \cite{Jegou:2010Product} and \acrshort{hnsw} \cite{Malkov:2018Efficient}. The \emph{dbADAM} component was complemented by \emph{wsADAM}, which could orchestrate query execution over multiple PostgreSQL instances using the MapReduce paradigm \cite{Dean:2008Mapreduce}. However, while ADAM was a great prototype, the physical data and storage model of PostgreSQL became a limiting factor for query execution performance on large data collections despite the ability for distribution.

ADAM's successor \adampro{} \cite{Giangreco:2016Adam} tried to address these limitations with a multi-model database system approach that allowed for query execution on different storage engines and distribution of data and computation using Apache Spark \footnote{See https://spark.apache.org/}, a large-scale data analytics platform. \adampro{} still relied on PostgreSQL for storage of classical, relational data but used Apache Parquet \footnote{See https://parquet.apache.org/} to store and access feature vectors. The work on \adampro was complemented by a systematic, theoretical model \cite{Giangreco:2018Database}: Using the relationship between (diss-)similarity and distance, Giangreco et al. demonstrated that for a database system to be able to support similarity search given the relational model as described in \cref{section:relational_data_model}, one can extend the set system of allowed data domains $\mathbb{D}$ by $\domain \subset \symreal^{dim}, dim \in \symnatural_{>1}$ and postulate the existence of a relational similarity operator $\tau_{\delta(\cdot,\cdot),a,q}(\relation)$ that \emph{``performs a similarity query under a distance $\delta(\cdot,\cdot)$ applied on an attribute $a$ of relation $\relation$ and compared to a query vector $q$.''} \cite{Giangreco:2018Database} (p. 138). According to the definition, such an operation introduces an implicit attribute in the underlying relation $\relation$, which in turn induces an ascending ordering of the tuples. Two variants of $\tau$ were proposed, namely $\tau^{kNN}_{\delta(\cdot,\cdot),a,q}$ (\acrshort{nns}), and $\tau^{\epsilon NN}_{\delta(\cdot,\cdot),a,q}$ (range search), which limit the number of retrieved results by their cardinality $k$ or a maximum cut-off distance $\epsilon$ respectively.

While postulating two new, relational operators $\tau^{kNN}_{\delta(\cdot,\cdot),a,q}$ or $\tau^{\epsilon NN}_{\delta(\cdot,\cdot),a,q}$ is well in tradition with how relational algebra has been extended over the years, it comes with limitations that become apparent upon disection of the operator's structure. In its postulated form, $\tau$ addresses several functions at once: It
\begin{enumerate*}[label=(\roman*)]
    \item specifies the distance function that should be evaluated,
    \item generates an implicit distance attribute on the underlying relation $\relation$,
    \item imposes an ascending ordering and limits the cardinality of $\relation$ based on a predicate or a specified limit.
\end{enumerate*}

While being very specific and thus straightforward to implement, the amalgamation of all this functionality into a single operation is very specifically tailored to the use case of \acrshort{nns} and range search and only of limited value when considering more general distance-based query operations. If one would, for example, want to obtain the $k$ farthest neighbours rather than the $k$ nearest neighbours, as necessary when doing MIPS or obtaining negative examples, we would have to either change the distance function, define new operators or extend the definition of $\tau$. 

Another important issue with the definition of $\tau$ in its proposed form is that despite the two operations $\tau^{kNN}_{\delta(\cdot,\cdot),a,q}$ and $\tau^{\epsilon NN}_{\delta(\cdot,\cdot),a,q}$ serving a very similar purpose and sharing the core definition, they behave very differently with respect to other operations. Generally, $\tau^{kNN}_{\delta(\cdot,\cdot),a,q}(\relation)$ does not commute with any selection $\sigma$ due to the inherent limiting of the cardinality to a constant value, hence:

\begin{equation}
    \label{equation:commutation_of_tau}
    \sigma(\tau^{kNN}_{\delta(\cdot,\cdot),a,q}(\relation)) \neq \tau^{kNN}_{\delta(\cdot,\cdot),a,q}(\sigma(\relation))
\end{equation}

The left-hand side of \cref{equation:commutation_of_tau} filters the results of a kNN-search on $\relation$, thus returning $n \leq k$ results, wherein $n = k$ only if $\sigma$ matches all tuples. The right-hand side of \cref{equation:commutation_of_tau} performs a kNN-search on a pre-filtered relation $\relation$, also returning $n \leq k$ entries. However, $n$ will only be smaller than $k$ if $\sigma$ selects fewer than $k$ tuples.

\section{FAISS and Milvus}

An important step towards an engine for scalable \acrshort{nns} was the publication of the open source library \acrfull{faiss} \cite{Johnson:2019Billion}. \acrshort{faiss} brings together different methods for \acrshort{anns} -- for example, \acrshort{pq} \cite{Jegou:2010Product}, \acrshort{hnsw} \cite{Malkov:2018Efficient} and \acrshort{lsh} \cite{Indyk1998:Approximate} based index structures -- and provides  \acrshort{simd} and \acrshort{gpu} support both for indexing and querying. While \acrshort{faiss} is not a multimedia database system per-se, it provides a formidable foundation for building one.

Another database system that considers itself a ``vector database built for scalable similarity search'' \footnote{See https://milvus.io/, accessed in June 2022} is called Milvus \cite{Wang:2021Milvus}. While the theoretical foundation behind it is not quite transparent, it claims to support hybrid queries involving both similarity search and predicates on scalar data types such as strings, booleans, integer and floating-point numbers as well as index support for both types of queries.

Most importantly, however, Milvus provides efficient \acrshort{nns} through state-of-the-art libraries and approaches namely FAISS \cite{Johnson:2019Billion}, ScANN \cite{Guo:2020Accelerating} and Spotify's Annoy \footnote{See https://github.com/spotify/annoy/, accessed in June 2022}, offers \acrshort{simd} and \acrshort{gpu} support and sports various indexes such as \acrshort{hnsw} or \acrshort{pq}. Dynamic data is supported by partitioning the data into segments and (re-)building of indexes on a segment per segment basis.