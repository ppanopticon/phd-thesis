\chapter{Multimedia Data Management}
\epiquote{A library is thought in cold storage.}{Herbert Samuel, Undated}
\label{chapter:theory_multimedia_database}

The insight that special requirements must be addressed to accomodate multimedia data in traditional \acrshort{dbms} can be traced back to the 1990s \cite{Marcus:1996Foundations,Adjeroh:1997Multimedia} with many theoretical contributions but very little in terms of concrete (open-source) implementations. However, from a database research perspective, the topic received renewed attention with the publication of the \emph{Lowell Database Research Self-Asessment} in 2005 \cite{Abiteboul:2005Lowell} and the \emph{Claremont Report on Database Research} in 2008 \cite{Agrawal:2008Claremont}, both of which identified support for unstructured- and multimedia data as well as the integration of information retrieval and database functionality as important research directions to consider.

In contrast, multimedia retrieval has traditionally relied on highly optimised data structures for storage and retrieval that were typically tightly coupled with the retrieval system itself in a monolithic architecture, offering little to no support for advanced data management functionalty. It was not until the \emph{Ten Research Questions for Scalable Multimedia Analytics} \cite{Jonson:2016Ten} that the topic of scalable data management for multimedia analytics and a convergence with database research was propose in the face of growing data collections \cite{Berns:2019V3C1,Rossetto:2021Insights}. Interestingly, many of the points raised by by JÃ³nsson et al. are somehow related to aspects listed in the 2014 \emph{The Beckman Report on Database Research} \cite{Abadi:2014Beckman}, namely the focus on ``end-to-end processing systems'' and ``data understanding'', demonstrating, that the two fields do have much in common.

However, while the amalgamation of text retrieval with classical database functionality became an topic of interest, multimedia retrieval remained a field of research largely separated from that of database research with the exception of a few, isolated contributions that tried to converge specific aspects such as scoring and ranking \cite{Chengkai:2005RankSQL,Zhang:2006Boolean}, extensions to relational algebra based on fuzzy logic \cite{Montesi:1999Similarity} or the conception of a multimedia query language \cite{Budikova:2012Query}.

\section{Early Multimedia Retrieval Systems}

As an alternative to a monolithic architecture, some of the early multimedia retrieval systems relied on and potentially combined existing database systems and built the special requirements for multimedia retrieval on top in either a single software-component (2-tier architecture) or some type of middleware that could then be used by other systems (3-tier architecture). Examples that use the 2-tier approach include systems such as the video retrieval system OVID \cite{Oomoto:1993OVID}, the image retrieval system MARS \cite{Rui:1997Content} or systems such as QBIC \cite{Flickner:1995Query} or MUVIS \cite{Kiranyaz:2003Muvis}. Unfortunately, neither of these systems are publicly available and therefore could not be examined in detail.

A prominent example is IBM's QBIC system \cite{Flickner:1995Query}, which allowed for (visual) similarity search on image and video data. QBIC's system architecture indicates the existence of a dedicated database layer used for storing features. However, there is little detail as to what type of database was used and what tasks were executed at a database level. Judging from the architecture and description, however, it seems that the queries were rather processed by the ``matching engine'' and the database itself was only used for storage. What is noteworthy is, that QBIC strictly distinguished between ``database population'' and ``database query'' as two different phases that took place independently of one another. A distinction rather atypical for tradtional database applications.

MUVIS \cite{Kiranyaz:2003Muvis} is another 2-tier multimedia retrieval system, which, in contrast to the aforementioned systems, combined multiple databases for storing image, video and audio information separately without, however, providing an explicit reason for doing so. Again, though, there is little detail as to what tasks fell to these database layers and which tasks were carried out by the browsing layer built on top of it. Interestingly, we can again observe the distinction between ``indexing'' and ``retrieval'' as two separate and independent phases in the system's lifecycle.

The \emph{Garlic Approach} \cite{Carey:1995Towards} proposed by IBM is an example of a 3-tier architecture. The Garlic middleware combines and integrates different, dedicated types of datastores to support heterogenous data under an extensible and modular query and runtime system. This system can then be used by other systems via standardised APIs. It was later used to integrate QBIC \cite{Cody:1995Querying}, which allowed for the combined search in visual and non-visual data. In a manner of speaking, Garlic can be seen as an early attempt at a polystore approach for multimedia data, like, for example \emph{Polypheny DB} \cite{Vogt:2018Polypheny}. 

The 2-tier architecture involving classical databases for information and multimedia retrieval is still very common today, with a tendency towards the use of more recent NoSQL systems \cite{Muhleisen:2014Old,Oliveira:2017Performance}. Furthermore, there has been some work on retrofitting established \acrshort{dbms} for multimedia retrieval, either by using existing extension endpoints (e.g., user-defined data types or \acrfull{udf}) or by writing dedicated extensions or altering core system components \cite{Guliato:2009PostgreSQL,Whang:2010Tightly,Giangreco:2014Adam,Fleites:2013Efficient,Whang:2015DB,Yang:2020Pase}.

\section{Libraries for Multimedia Retrieval}
\label{section:nns_libraries}

Instead of tightly integrating multimedia retrieval techniques with systems that use them, the past two decades have seen a trend towards open source libraries that could be (re-)used by others. The standardised \acrshort{anns} benchmarking campaign known as \emph{ANN-Benchmarks} \cite{Aumueller:2017ANN} \footnote{See http://ann-benchmarks.com/, Accessed July 2022}, for example, lists and evaluates 25 individual libraries, such as, \emph{Spotify Annoy} \footnote{See https://github.com/spotify/annoy, Accessed July 2022}, ScaNN \cite{Guo:2020Accelerating} by Google Research, \emph{Hnswlib} \cite{Malkov:2018Efficient}, Facebook's \acrshort{faiss} \cite{Johnson:2019Billion} or Yahoo's NGT \cite{Iwasaki2016:Pruned}.

A noteworthy, early attempt at providing such a library for content-based image retrieval was \acrfull{lire} \cite{Luc:2008LIRE} -- an open source Java library based on the Apache Lucene engine, which is typically used for fulltext search. \acrshort{lire} was later used and extended to create \emph{LIvRE} \cite{Oliveira:2016Large}, a similar project but for video retrieval. While both \acrshort{lire} and LIvRE were an important step in the right direction, they unfortunately tightly integrated feature extraction and storage and were thus of very limited use when trying to use different types of features. Nonetheless, these project demonstrated Apache Lucene's capability of executing \acrshort{nns}. Consequently, Lucene based projects such as ElasticSearch \footnote{See https://www.elastic.co/, Accessed July 2022} or OpenSearch \footnote{See https://opensearch.org/, Accessed July 2022} still provide \acrshort{nns} and \acrshort{anns} through plugins and extensions such as Elastiknn \footnote{See https://github.com/erikbern/ann-benchmarks/, Accessed July 2022}.

An important step towards an engine for scalable \acrshort{nns} was the open source library \acrfull{faiss} \cite{Johnson:2019Billion}. \acrshort{faiss} brings together different methods for \acrshort{anns} -- for example, \acrshort{pq} \cite{Jegou:2010Product}, \acrshort{hnsw} \cite{Malkov:2018Efficient} and \acrshort{lsh} \cite{Indyk1998:Approximate} based index structures -- and provides  \acrshort{simd} and \acrshort{gpu} support both for indexing and querying. While \acrshort{faiss} is not a multimedia database system per-se, it provides a formidable foundation for building one.

While all these projects are important and useful, they typically require integration into a system or framework and do not provide an ``out-of-the-box'' solution that can be used via standardised interfaces such as \acrshort{sql}.

\section{Vector Database Engines}
\label{section:vector_databases}
The past few years have seen the creation of novel vector databases as either dedicated systems or extensions to existing ones. We refrain from presenting all these systems in detail and instead briefly name and describe a few, with the exception of one system that will receive an more detailed discussion. All systems have in common that they have been built as cloud-native applications with distribution in mind and that they are open source. For every system, we reference the respective GitHub repository as well as so the current version and the time of the last update to illustrate the activity in this particular domain.

\begin{description}
    \item[Weaviate (1.14.1, July 2022)] \footnote{See https://github.com/semi-technologies/weaviate/, Accessed July 2022} is a vector search engine that allows the storage of vectors and attributes. Both the vector and attributes can be queried using the \emph{GraphQL} query language and it relies on \acrshort{hnsw} \cite{Malkov:2018Efficient} for \acrshort{anns}. Weaviate claims full \acrshort{crud} support, i.e., changes to entries are possible, which however limits the types of index structures used (currently, a custom \acrshort{hnsw} index is employed). The recent update to version 1.5.0 has added support the Euclidean distance and the dot product between vectors.
    \item[Vald (1.5.5, July 2022)] \footnote{See https://github.com/vdaas/vald/, Accessed July 2022} is a vector search engine that relies on \acrshort{ngt} for \acrshort{anns}. It allows for similarity search as well as lookup's by identifier using a gRPC based query API. A unique selling point is the \emph{asynchronous auto indexing} feature, which refreshes outdated indexes in the background in a non-blocking fashion, which allows for changes to the data.
    \item[Qdrant (0.8.5, July 2022)] \footnote{See https://github.com/qdrant/qdrant/, Accessed July 2022} is a similarity search engine that can be used to store high-dimensional vectors with additional attributes -- called payload. It can be characterised as document database. Qdrant supports similarity search queries as well as filtering for payload attributes via either a gRPC or RESTful interface. Both payload attributes as well as vectors can be indexed and \acrshort{hnsw} \cite{Malkov:2018Efficient} is used for the latter. A query planner decides about query execution in case multiple index structures are employed.
\end{description}


\subsection{Milvus}

Milvus (2.0.2, April 2022) \cite{Wang:2021Milvus} is vector database system with its most recent 2.0 release dating back to January 2022. Milvus considers itself a ``vector database built for scalable similarity search'' and it relies on many of the libraries mentioned in \Cref{section:nns_libraries}. According to information on the official blog \footnote{See https://milvus.io/blog/, Accessed July 2022}, it was built as a cloud-native application with high scalability and availability in mind. It offers support for functionalty that goes beyond mere \acrshort{nns}, such as, Boolean filtering as well as hybrid queries involving both similarity search and Boolean predicates on scalar data types as well as index support for both types of queries. In addition, Milvus is also capable of processing online changes to data (i.e., inserts and deletes) using an eventual consistency model and a type of \acrshort{mvcc} based snapshot isolation for queries, which also allows for queries on snapshots at different points in time (time travel).

Data in Milvus is organised into named collections, which can consist of multiple attributes and are distributed among multiple partitions. The logical data model and type system is similar to that of \cottontail{}, however, in addition to scalar types (e.g., \texttt{int}, \texttt{float}, \texttt{double} or \texttt{string}) it only allows for \texttt{float} or binary vectors. Milvus offers client libraries for Go, Java (and Kotlin), Python and Node, which offers a user-friendly facade over the gRPC-based query interface and allows for data definition, data management and querying.

Milvus provides efficient \acrshort{nns} through a component named \emph{Knowwhere}, which provides a standardised operations interface that abstracts away the underlying state-of-the-art libraries and techniques such as FAISS \cite{Johnson:2019Billion}, Hnswlib \cite{Malkov:2018Efficient} and Spotify's Annoy \footnote{See https://github.com/spotify/annoy/, accessed in June 2022} and which offers suport for \acrshort{simd} and \acrshort{gpu} execution as well as various index structures. Dynamic data support for indexes is enabled eventual consistency and by (re-)building of indexes on a partition per partition basis. Furthermore, deletes are filtered on the fly during query time based on so called \emph{tombstone} markers in the database, a concept known from Apache Cassandra.

It is important to mention that while data held by Milvus resides on disk, in order to query a collection it must first be loaded into memory. Therefore, for Milvus to be able to execute \acrshort{nns}, all the data (or at least the indexes), must fit into main memory. This can become problematic, especially if multiple collections are queried on a regular basis.

\section{Databases for Data Science}
With the start of the NoSQL movement some researchers began to address the shortcomings of the relational data model in the context of scientific applications and proposed different architectures, such as array databases. Early examples include SimDB \cite{Silva:2010SimDB} or SciDB \cite{Stonebraker:2011Architecture,Stonebraker:2013SciDB}. The topic of science oriented databases has has been listed as highly-relevant in the 2020 database research self-assessment report, due to the continued trend towards data driven applications, data science and data analytics \cite{Abadi:2020Seattle}. In addition to challenges in the domain of data governance, one focus that was identified is the convergence between linear algebra and relational algebra in an efficient and effective framework \cite{Luo:2018Scalable}, an effort that could also greatly benefit multimedia retrieval and analytics applications \cite{Berry:1995Using}. Another area of interest is the use of specialised hardware such as \acrshort{fpga}s or \acrshort{gpu}s, to accelerate query evaluation.
 
\section{vitrivr, ADAM and \texorpdfstring{\adampro{}}{ADAMpro}}

To the best of our knowledge, the open source \vitrivr{} video retireval \cite{Rossetto:2016vitrivr} and later multimedia retrieval \cite{Gasser:2019Multimodal} system was one of the first systems to advertise a 3-tier architecture that included a specialised multimedia database in addition to a dedicated feature extraction and fusion engine and a user interface. The first generation database layer came in the form of ADAM \cite{Giangreco:2014Adam}, which can be seen as an early attempt at bridging the gap between databases and multimedia retrieval. At its core, ADAM was an extension to PostgreSQL -- called \emph{dbADAM} -- that added support for \acrshort{nns} and \acrshort{vaf}-based indexes \cite{Weber:1998Va}. In that, it is comparable to projects like \emph{pgvector} \footnote{See https://github.com/pgvector/pgvector/, Accessed July 2022}, \emph{pgANN} \footnote{See https://github.com/netrasys/pgANN/, Accessed July 2022} and PASE \cite{Yang:2020Pase}, which however use more recent index structures such as \acrshort{pq} \cite{Jegou:2010Product} and \acrshort{hnsw} \cite{Malkov:2018Efficient}. The \emph{dbADAM} component was complemented by \emph{wsADAM}, which could orchestrate query execution over multiple PostgreSQL instances using the MapReduce paradigm \cite{Dean:2008Mapreduce}. ADAM's successor \adampro{} \cite{Giangreco:2016Adam} tried to address these limitations with a multi-model (polyglot) database system approach that allowed for query execution on different storage engines and distribution of data and computation using \emph{Apache Spark}, a large-scale data analytics platform. \adampro{} still relied on PostgreSQL for storage of classical, relational data but used \emph{Apache Parquet} to store and access feature vectors. 

The work on \adampro{} was complemented by a theoretical model \cite{Giangreco:2018Database}: Giangreco et al. demonstrated that for a database system to be able to support similarity search given the relational model as described in \cref{section:relational_data_model}, one can extend the set system of allowed data domains $\mathbb{D}$ by $\domain \subset \symreal^{dim}, dim \in \symnatural_{>1}$ and postulate the existence of a relational similarity operator $\tau_{\delta(\cdot,\cdot),a,q}(\relation)$ that \emph{``performs a similarity query under a distance $\delta(\cdot,\cdot)$ applied on an attribute $a$ of relation $\relation$ and compared to a query vector $q$.''} \cite{Giangreco:2018Database} (p. 138). According to the definition, such an operation introduces an implicit attribute in the underlying relation $\relation$, which in turn induces an ascending ordering of the tuples. Two variants of $\tau$ were proposed, namely $\tau^{kNN}_{\delta(\cdot,\cdot),a,q}$ (\acrshort{nns}), and $\tau^{\epsilon NN}_{\delta(\cdot,\cdot),a,q}$ (range search), which select tuples by their cardinality $k$ or a maximum cut-off distance $\epsilon$ respectively.

While postulating two new, relational operators is well within tradition of how relational algebra has been extended over the years (see \Cref{section:rel_extensions}), it comes with certain limitations. In its proposed form, $\tau$ addresses several functions at once: It
\begin{enumerate*}[label=(\roman*)]
    \item specifies the distance function that should be evaluated,
    \item generates an implicit distance attribute on the underlying relation $\relation$,
    \item imposes an ascending ordering and limits the cardinality of $\relation$ based on a predicate or a specified limit.
\end{enumerate*}

While being straightforward to implement, the amalgamation of all this functionality into a single operation is very specifically tailored to the use case of \acrshort{nns} and range search and only of limited value when considering more general distance-based query operations. If one were to, for example, obtain the $k$ farthest neighbours rather than the $k$ nearest neighbours, as necessary when doing \acrshort{mips} or obtaining negative examples, they would have to either change the distance function, define new operators or extend the definition of $\tau$. 

Another important issue with the definition of $\tau$ in its proposed form is that despite the two operations $\tau^{kNN}_{\delta(\cdot,\cdot),a,q}$ and $\tau^{\epsilon NN}_{\delta(\cdot,\cdot),a,q}$ serving a very similar purpose and sharing the core definition, they behave very differently with respect to other operations. Generally, $\tau^{kNN}_{\delta(\cdot,\cdot),a,q}(\relation)$ does not commute with any selection $\sigma$ due to the inherent limiting of the cardinality to a constant value, hence:

\begin{equation}
    \label{equation:commutation_of_tau}
    \sigma(\tau^{kNN}_{\delta(\cdot,\cdot),a,q}(\relation)) \neq \tau^{kNN}_{\delta(\cdot,\cdot),a,q}(\sigma(\relation))
\end{equation}

The left-hand side of \cref{equation:commutation_of_tau} filters the results of a kNN-search on $\relation$, thus returning $n \leq k$ results, wherein $n = k$ only if $\sigma$ matches all tuples. The right-hand side of \cref{equation:commutation_of_tau} performs a kNN-search on a pre-filtered relation $\relation$, also returning $n \leq k$ entries. However, $n$ will only be smaller than $k$ if $\sigma$ selects fewer than $k$ tuples.