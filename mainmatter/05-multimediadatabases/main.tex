\chapter{Multimedia Data Management}
\epiquote{A library is thought in cold storage.}{Herbert Samuel, Undated}
\label{chapter:theory_multimedia_database}

The insight that special requirements must be addressed to accomodate multimedia data in traditional \acrshort{dbms} can be traced back to the 1990s \cite{Marcus:1996Foundations,Adjeroh:1997Multimedia} with many theoretical contributions but very little in terms of concrete (open-source) implementations. However, from a database research perspective, the topic received renewed attention with the publication of the \emph{Lowell Database Research Self-Asessment} in 2005 \cite{Abiteboul:2005Lowell} and the \emph{Claremont Report on Database Research} in 2008 \cite{Agrawal:2008Claremont}, both of which identified support for unstructured- and multimedia data as well as the integration of information retrieval and database functionality as important research directions to consider.

In contrast, multimedia retrieval has traditionally relied on highly optimised data structures for storage and retrieval that were typically tightly coupled with the retrieval system itself in a monolithic architecture with little to no support for dedicated data management. It was not until the \emph{Ten Research Questions for Scalable Multimedia Analytics} \cite{Jonson:2016Ten} that the topic of scalable data management for multimedia analytics and a convergence with database research was identified, especially in the context of growing data collections \cite{Berns:2019V3C1,Rossetto:2021Insights}. Interestingly, many of the points raised by by JÃ³nsson et al. are somehow related to aspects mentioned in the 2014 \emph{The Beckman Report on Database Research} \cite{Abadi:2014Beckman}, namely the focus on ``end-to-end processing systems'' and ``data understanding'', demonstrating, that the two fields do have much in common.

However, while the amalgamation of text retrieval with classical database functionality remained an topic of interest, multimedia retrieval remained a field of research largely separated from that of database research with the exception of a few, isolated contributions that tried to converge specific aspects such as scoring and ranking \cite{Chengkai:2005RankSQL,Zhang:2006Boolean}, extensions to relational algebra based on fuzzy logic \cite{Montesi:1999Similarity} or the conception of a multimedia query language \cite{Budikova:2012Query}.

\section{Early Multimedia Retrieval Systems}

In addition to implementing a monolithic architecture, early multimedia retrieval systems tended to rely on and potentially combine existing database systems and to build the special requirements for multimedia retrieval on top in either a single software-component (2-tier architecture) or some type of middleware that could then be used by other systems (3-tier architecture). Examples that use the 2-tier approach include systems such as the video retrieval system OVID \cite{Oomoto:1993OVID}, the image retrieval system MARS \cite{Rui:1997Content} or systems such as QBIC \cite{Flickner:1995Query} or MUVIS \cite{Kiranyaz:2003Muvis}. Unfortunately, neither of these systems are publicly available and therefore could not be examined in detail.

IBM's QBIC system \cite{Flickner:1995Query} allowed for (visual) similarity search on image and video data. QBIC's system architecture indicates the existence of a dedicated database layer used for storing features. However, there is little detail as to what type of database was used and what tasks were executed at a database level. Judging from the architecture and description, however, it seems that the queries were rather processed by the ``matching engine'' and the database itself was only used for storage. What is noteworthy is, that QBIC strictly distinguished between ``database population'' and ``database query'' as two different phases that took place independently of one another. A distinction very atypical for tradtional database applications.

MUVIS \cite{Kiranyaz:2003Muvis} is another 2-tier multimedia retrieval system, which, in contrast to the aforementioned systems, combined multiple databases for storing image, video and audio information separately without, however, providing an explicit reason for doing so. Again, though, there is little detail as to what tasks fell to these database layers and which tasks were carried out by the browsing layer built on top of it. Interestingly, we can again observe the distinction between ``indexing'' and ``retrieval'' as two separate and independent phases in the system's lifecycle.

The \emph{Garlic Approach} \cite{Carey:1995Towards} proposed by IBM is an example of a 3-tier architecture. The Garlic middleware tries to combine and integrate different, dedicated types of datastores to support heterogenous data under an extensible and modular query and runtime system. This system can then be used by other systems via standardised APIs. It was later used to integrate QBIC \cite{Cody:1995Querying}, which allowed for the combined search in visual and non-visual data. In a manner of speaking, Garlic can be seen as an early attempt at a polystore approach for multimedia data \cite{Vogt:2018Polypheny}. 

The 2-tier architecture involving classical databases for information and multimedia retrieval is still very common today, with a tendency towards NoSQL systems \cite{Muhleisen:2014Old,Oliveira:2017Performance}. Furthermore, there has been some work on extending existing \acrshort{dbms}, either by using provided extension endpoints (e.g., plugins, user-defined data types or used-defined functions) or by altering the systems themselves \cite{Whang:2010Tightly,Giangreco:2014Adam,Whang:2015DB,Yang:2020Pase}.

\section{Libraries for Multimedia Retrieval}

Instead of tightly integrating multimedia retrieval techniques with systems that use them, the past two decades have seen a trend towards open source libraries that could be (re-)used by other systems. For example, the standardised benchmarking campaign for \acrshort{anns} known as ANN-Benchmarks \cite{Aumueller:2017ANN} \footnote{See http://ann-benchmarks.com/, Accessed July 2022} lists and evaluates 25 individual libraries. Examples include \emph{Spotify Annoy} \footnote{See https://github.com/spotify/annoy, Accessed July 2022}, ScaNN \cite{Guo:2020Accelerating} by Google Research, \emph{Hnswlib} \cite{Malkov:2018Efficient} or Facebook's FAISS \cite{Johnson:2019Billion}.

A noteworthy attempt at providing a data management layer for content-based image retrieval was \acrfull{lire} \cite{Luc:2008LIRE} -- an open source Java library based on the Apache Lucene \footnote{See https://lucene.apache.org/, Accessed July 2022} engine, which is typically used for fulltext search. \acrshort{lire} later used and extended to create \emph{LIvRE} \cite{Oliveira:2016Large}, a similar project but for video retrieval. While both \acrshort{lire} and LIvRE were an important step in the right direction, they unfortunately tightly integrated feature extraction and storage and were thus of very limited use when trying to use different types of features. Nonetheless, these project demonstrated Apache Lucene's capability of executing \acrshort{nns}. Consequently, Lucene based projects such as ElasticSearch \footnote{See https://www.elastic.co/, Accessed July 2022} or OpenSearch \footnote{See https://opensearch.org/, Accessed July 2022} still provide \acrshort{nns} and \acrshort{anns} through plugins and extensions such as Elastiknn \footnote{See https://github.com/erikbern/ann-benchmarks/, Accessed July 2022}.

An important step towards an engine for scalable \acrshort{nns} was the open source library \acrfull{faiss} \cite{Johnson:2019Billion}. \acrshort{faiss} brings together different methods for \acrshort{anns} -- for example, \acrshort{pq} \cite{Jegou:2010Product}, \acrshort{hnsw} \cite{Malkov:2018Efficient} and \acrshort{lsh} \cite{Indyk1998:Approximate} based index structures -- and provides  \acrshort{simd} and \acrshort{gpu} support both for indexing and querying. While \acrshort{faiss} is not a multimedia database system per-se, it provides a formidable foundation for building one.

While all these projects are important and useful, they typically require integration into a system or framework and do not provide an ``out-of-the-box'' solution that can be used via standardised interfaces such as \acrshort{sql}.

\section{Milvus the Vector Database}
Milvus \cite{Wang:2021Milvus} is a rather recent system that uses many of the aforementioned contributions. Milvus considers itself a ``vector database built for scalable similarity search'' \footnote{See https://milvus.io/, accessed in June 2022} and is an open-source system. While the theoretical foundation behind it is not quite transparent, it offers support for functionalty that goes beyond mere \acrshort{nns}, such as, hybrid queries involving both similarity search and Boolean predicates on scalar data types such as strings, booleans, integer and floating-point numbers as well as index support for both types of queries. An example of such a hybrid query is provided in \Cref{listing:milvus_query}. In addition to similarity and Boolean search, Milvus is also capable of processing online changes to data and supports different consistency levels for queries through a type of \acrshort{mvcc} which also allows for queries on snapshots at different points in time.

\begin{lstlisting}[language=Python, caption={Example of a hybrid query to Milvus in Python. The \texttt{expr} parameter can be used to specify Boolean filters. Source: https://milvus.io/}, label={listing:milvus_query}]
    from pymilvus import Collection
    
    # Get an existing collection.
    collection = Collection("images")      
    collection.load()

    # Perform search
    sparams = {"metric_type": "L2", "params": {"nprobe": 10}}
    results = collection.search(
        data=[[0.0, 0.0]], anns_field="feature", param=sparams, 
        limit=10, expr="size <= 1000", consistency_level="Strong"
    )
\end{lstlisting}

Most importantly, however, Milvus provides efficient \acrshort{nns} through state-of-the-art libraries and techniques such as FAISS \cite{Johnson:2019Billion}, ScANN \cite{Guo:2020Accelerating} and Spotify's Annoy \footnote{See https://github.com/spotify/annoy/, accessed in June 2022}, offers \acrshort{simd} and \acrshort{gpu} support and sports various indexes such as \acrshort{hnsw} \cite{Malkov:2018Efficient} or \acrshort{pq} \cite{Jegou:2010Product}. Dynamic data for indexing is supported by partitioning the data into segments and (re-)building of indexes on a segment per segment basis.

\section{Databases for Data Science}
It was not until the start of the NoSQL movement, that some researchers began to address the shortcomings of the relational data model in the context of multimedia and particularily scientific applications. Especially the latter has been identified as a highly-relevant topic due to the continued trend towards data driven application and data science and analytics \cite{Abadi:2020Seattle}. Early examples for such databases include systems such as SimDB \cite{Silva:2010SimDB} or SciDB \cite{Stonebraker:2013SciDB}.
 
\section{vitrivr, ADAM and \texorpdfstring{\adampro{}}{ADAMpro}}

To the best of our knowledge, the open source \vitrivr{} video retireval \cite{Rossetto:2016vitrivr} and later multimedia retrieval \cite{Gasser:2019Multimodal} system was one of the first systems to advertise a 3-tier architecture that included a dedicated multimedia database in addition to a feature extraction and fusion engine and a user interface. The first generation database layer came in the form of ADAM \cite{Giangreco:2014Adam}, which can be seen as an early attempt at bridging the gap between databases and multimedia retrieval. ADAM was basically an extension to PostgreSQL -- called \emph{dbADAM} -- that added support for \acrshort{nns} and \acrshort{vaf}-based indexes \cite{Weber:1998Va}. In that, it is comparable to projects like \emph{pgvector} \footnote{See https://github.com/pgvector/pgvector/} and PASE \cite{Yang:2020Pase}, which however use more up-to-date index structures such as \acrshort{pq} \cite{Jegou:2010Product} and \acrshort{hnsw} \cite{Malkov:2018Efficient}. The \emph{dbADAM} component was complemented by \emph{wsADAM}, which could orchestrate query execution over multiple PostgreSQL instances using the MapReduce paradigm \cite{Dean:2008Mapreduce}. However, while ADAM was a great prototype, the physical data and storage model of PostgreSQL became a limiting factor for query execution performance on large data collections despite the ability for distribution.

ADAM's successor \adampro{} \cite{Giangreco:2016Adam} tried to address these limitations with a multi-model (polyglot) database system approach that allowed for query execution on different storage engines and distribution of data and computation using Apache Spark \footnote{See https://spark.apache.org/}, a large-scale data analytics platform. \adampro{} still relied on PostgreSQL for storage of classical, relational data but used Apache Parquet \footnote{See https://parquet.apache.org/} to store and access feature vectors. The work on \adampro{} was complemented by a systematic, theoretical model \cite{Giangreco:2018Database}: By leveraging the relationship between (diss-)similarity and distance, Giangreco et al. demonstrated that for a database system to be able to support similarity search given the relational model as described in \cref{section:relational_data_model}, one can extend the set system of allowed data domains $\mathbb{D}$ by $\domain \subset \symreal^{dim}, dim \in \symnatural_{>1}$ and postulate the existence of a relational similarity operator $\tau_{\delta(\cdot,\cdot),a,q}(\relation)$ that \emph{``performs a similarity query under a distance $\delta(\cdot,\cdot)$ applied on an attribute $a$ of relation $\relation$ and compared to a query vector $q$.''} \cite{Giangreco:2018Database} (p. 138). According to the definition, such an operation introduces an implicit attribute in the underlying relation $\relation$, which in turn induces an ascending ordering of the tuples. Two variants of $\tau$ were proposed, namely $\tau^{kNN}_{\delta(\cdot,\cdot),a,q}$ (\acrshort{nns}), and $\tau^{\epsilon NN}_{\delta(\cdot,\cdot),a,q}$ (range search), which limit the number of retrieved results by their cardinality $k$ or a maximum cut-off distance $\epsilon$ respectively.

While postulating two new, relational operators $\tau^{kNN}_{\delta(\cdot,\cdot),a,q}$ or $\tau^{\epsilon NN}_{\delta(\cdot,\cdot),a,q}$ is well in tradition of how relational algebra has been extended over the years (see \Cref{section:rel_extensions}), it comes with certain limitations. In its proposed form, $\tau$ addresses several functions at once: It
\begin{enumerate*}[label=(\roman*)]
    \item specifies the distance function that should be evaluated,
    \item generates an implicit distance attribute on the underlying relation $\relation$,
    \item imposes an ascending ordering and limits the cardinality of $\relation$ based on a predicate or a specified limit.
\end{enumerate*}

While being straightforward to implement, the amalgamation of all this functionality into a single operation is very specifically tailored to the use case of \acrshort{nns} and range search and only of limited value when considering more general distance-based query operations. If one were to, for example, obtain the $k$ farthest neighbours rather than the $k$ nearest neighbours, as necessary when doing \acrshort{mips} or obtaining negative examples, they would have to either change the distance function, define new operators or extend the definition of $\tau$. 

Another important issue with the definition of $\tau$ in its proposed form is that despite the two operations $\tau^{kNN}_{\delta(\cdot,\cdot),a,q}$ and $\tau^{\epsilon NN}_{\delta(\cdot,\cdot),a,q}$ serving a very similar purpose and sharing the core definition, they behave very differently with respect to other operations. Generally, $\tau^{kNN}_{\delta(\cdot,\cdot),a,q}(\relation)$ does not commute with any selection $\sigma$ due to the inherent limiting of the cardinality to a constant value, hence:

\begin{equation}
    \label{equation:commutation_of_tau}
    \sigma(\tau^{kNN}_{\delta(\cdot,\cdot),a,q}(\relation)) \neq \tau^{kNN}_{\delta(\cdot,\cdot),a,q}(\sigma(\relation))
\end{equation}

The left-hand side of \cref{equation:commutation_of_tau} filters the results of a kNN-search on $\relation$, thus returning $n \leq k$ results, wherein $n = k$ only if $\sigma$ matches all tuples. The right-hand side of \cref{equation:commutation_of_tau} performs a kNN-search on a pre-filtered relation $\relation$, also returning $n \leq k$ entries. However, $n$ will only be smaller than $k$ if $\sigma$ selects fewer than $k$ tuples.