\chapter{Multimedia Data Management}
\label{chapter:theory_multimedia_database}

The insight that special requirements must be addressed to accomodate multimedia data in traditional \acrshort{dbms} can be traced back to the 1990s \cite{Marcus:1996Foundations,Adjeroh:1997Multimedia} with many theoretical contributions but very little in terms of concrete (open-source) implementations. Instead, multimedia retrieval remained a field of research largely separated from that of database research with the exception of a few, isolated contributions that tried to converge specific aspects of the fields such as scoring and ranking \cite{Chengkai:2005RankSQL,Zhang:2006Boolean} or fuzzy relational algebra \cite{Montesi:1999Similarity}.

If we go back and explore how multimedia retrieval systems operated in the early days, we are likely to arrive at IBM's QBIC system \cite{Flickner:1995Query}, which allowed for (visual) similarity search on image and video data. QBIC's system architecture hints at the existence of a dedicated database layer used for storing features. However, there is little detail as to what type of database was used and what tasks were executed at a database level. Judging from the architecture and description, it seems that the queries were rather processed by the ``matching engine'' and the database itself was only used for storage. Furthermore, QBIC strictly distinguished between ``database population'' (offline) and ``database query'' (online) phases, as did many systems thereafter.

Similarly, the MUVIS \cite{Kiranyaz:2003Muvis} system employed multiple databases for storing image, video and audio information separately without providing an explicit reason for this mode of operation. However, there is little detail as to what tasks fell to these database layers and which tasks were carried out by the ``MBrowser'' layer built on top of it. Interestingly, we can again observe the distinction between ``indexing'' (offline) and ``retrieval'' (online) for MUVIS as well. Unfortunately, neither the QBIC nor the MUVIS system are publicly available and therefore the inner workings cannot be examined.

A noteworthy attempt at providing a data management layer for content-based image retrieval was \acrfull{lire} \cite{Luc:2008LIRE}, which is an open source Java library based on the Apache Lucene \footnote{See https://lucene.apache.org/} engine, which is typically used for fulltext search and retrieval. While \acrshort{lire} was a step in the right direction, it still tightly integrated feature extraction and storage and was thus of very limited use when trying to use different types of features. It was not until the start of the NoSQL movement, that some began to build solutions to the shortcomings of the relational model in the context of multimedia and scientific applications \cite{Silva:2010SimDB,Stonebraker:2013SciDB}.

\section{vitrivr, ADAM and \texorpdfstring{\adampro{}}{ADAMpro}}

To the best of our knowledge, the open source \vitrivr{} video retireval \cite{Rossetto:2016vitrivr} and later multimedia retrieval \cite{Gasser:2019Multimodal} system was one of the first systems to advertise a 3-tier architecture that included a dedicated multimedia database in addition to a feature extraction and fusion engine and a user interface. The first generation database layer came in the form of ADAM \cite{Giangreco:2014Adam}, which can be seen as an early attempt at bridging the gap between databases and multimedia retrieval. ADAM was basically an extension to PostgreSQL -- called \emph{dbADAM} -- that added support for \acrshort{nns} and \acrshort{vaf}-based indexes \cite{Weber:1998Va}. In that, it is comparable to projects like \emph{pgvector} \footnote{See https://github.com/pgvector/pgvector/} and PASE \cite{Yang:2020Pase}, which however use more up-to-date index structures such as \acrshort{pq} \cite{Jegou:2010Product} and \acrshort{hnsw} \cite{Malkov:2018Efficient}. The \emph{dbADAM} component was complemented by \emph{wsADAM}, which could orchestrate query execution over multiple PostgreSQL instances using the MapReduce paradigm \cite{Dean:2008Mapreduce}. However, while ADAM was a great prototype, the physical data and storage model of PostgreSQL became a limiting factor for query execution performance on large data collections despite the ability for distribution.

ADAM's successor \adampro{} \cite{Giangreco:2016Adam} tried to address these limitations with a multi-model database system approach that allowed for query execution on different storage engines and distribution of data and computation using Apache Spark \footnote{See https://spark.apache.org/}, a large-scale data analytics platform. \adampro{} still relied on PostgreSQL for storage of classical, relational data but used Apache Parquet \footnote{See https://parquet.apache.org/} to store and access feature vectors. The work on \adampro{} was complemented by a systematic, theoretical model \cite{Giangreco:2018Database}: By leveraging the relationship between (diss-)similarity and distance, Giangreco et al. demonstrated that for a database system to be able to support similarity search given the relational model as described in \cref{section:relational_data_model}, one can extend the set system of allowed data domains $\mathbb{D}$ by $\domain \subset \symreal^{dim}, dim \in \symnatural_{>1}$ and postulate the existence of a relational similarity operator $\tau_{\delta(\cdot,\cdot),a,q}(\relation)$ that \emph{``performs a similarity query under a distance $\delta(\cdot,\cdot)$ applied on an attribute $a$ of relation $\relation$ and compared to a query vector $q$.''} \cite{Giangreco:2018Database} (p. 138). According to the definition, such an operation introduces an implicit attribute in the underlying relation $\relation$, which in turn induces an ascending ordering of the tuples. Two variants of $\tau$ were proposed, namely $\tau^{kNN}_{\delta(\cdot,\cdot),a,q}$ (\acrshort{nns}), and $\tau^{\epsilon NN}_{\delta(\cdot,\cdot),a,q}$ (range search), which limit the number of retrieved results by their cardinality $k$ or a maximum cut-off distance $\epsilon$ respectively.

While postulating two new, relational operators $\tau^{kNN}_{\delta(\cdot,\cdot),a,q}$ or $\tau^{\epsilon NN}_{\delta(\cdot,\cdot),a,q}$ is well in tradition of how relational algebra has been extended over the years, it comes with limitations that become apparent upon disection of the operator's structure. In its proposed form, $\tau$ addresses several functions at once: It
\begin{enumerate*}[label=(\roman*)]
    \item specifies the distance function that should be evaluated,
    \item generates an implicit distance attribute on the underlying relation $\relation$,
    \item imposes an ascending ordering and limits the cardinality of $\relation$ based on a predicate or a specified limit.
\end{enumerate*}

While being straightforward to implement, the amalgamation of all this functionality into a single operation is very specifically tailored to the use case of \acrshort{nns} and range search and only of limited value when considering more general distance-based query operations. If one would, for example, want to obtain the $k$ farthest neighbours rather than the $k$ nearest neighbours, as necessary when doing MIPS or obtaining negative examples, we would have to either change the distance function, define new operators or extend the definition of $\tau$. 

Another important issue with the definition of $\tau$ in its proposed form is that despite the two operations $\tau^{kNN}_{\delta(\cdot,\cdot),a,q}$ and $\tau^{\epsilon NN}_{\delta(\cdot,\cdot),a,q}$ serving a very similar purpose and sharing the core definition, they behave very differently with respect to other operations. Generally, $\tau^{kNN}_{\delta(\cdot,\cdot),a,q}(\relation)$ does not commute with any selection $\sigma$ due to the inherent limiting of the cardinality to a constant value, hence:

\begin{equation}
    \label{equation:commutation_of_tau}
    \sigma(\tau^{kNN}_{\delta(\cdot,\cdot),a,q}(\relation)) \neq \tau^{kNN}_{\delta(\cdot,\cdot),a,q}(\sigma(\relation))
\end{equation}

The left-hand side of \cref{equation:commutation_of_tau} filters the results of a kNN-search on $\relation$, thus returning $n \leq k$ results, wherein $n = k$ only if $\sigma$ matches all tuples. The right-hand side of \cref{equation:commutation_of_tau} performs a kNN-search on a pre-filtered relation $\relation$, also returning $n \leq k$ entries. However, $n$ will only be smaller than $k$ if $\sigma$ selects fewer than $k$ tuples.

\section{FAISS and Milvus}

An important step towards an engine for scalable \acrshort{nns} was the publication of the open source library \acrfull{faiss} \cite{Johnson:2019Billion}. \acrshort{faiss} brings together different methods for \acrshort{anns} -- for example, \acrshort{pq} \cite{Jegou:2010Product}, \acrshort{hnsw} \cite{Malkov:2018Efficient} and \acrshort{lsh} \cite{Indyk1998:Approximate} based index structures -- and provides  \acrshort{simd} and \acrshort{gpu} support both for indexing and querying. While \acrshort{faiss} is not a multimedia database system per-se, it provides a formidable foundation for building one.

A recent multimedia database system based on FAISS that considers itself a ``vector database built for scalable similarity search'' \footnote{See https://milvus.io/, accessed in June 2022} is called Milvus \cite{Wang:2021Milvus}. Milvus is an open-source system and while the theoretical foundation behind it is not quite transparent, it supports hybrid queries involving both similarity search and Boolean predicates on scalar data types such as strings, booleans, integer and floating-point numbers as well as index support for both types of queries. An example of such a hybrid query is provided in \Cref{listing:milvus_query}. 

\begin{lstlisting}[language=Python, caption={Example of a hybrid query to Milvus in Python. The \texttt{expr} parameter can be used to specify Boolean filters. Source: https://milvus.io/}, label={listing:milvus_query}]
    from pymilvus import Collection
    
    # Get an existing collection.
    collection = Collection("images")      
    collection.load()

    # Perform search
    sparams = {"metric_type": "L2", "params": {"nprobe": 10}}
    results = collection.search(
        data=[[0.0, 0.0]], anns_field="feature", param=sparams, 
        limit=10, expr="size <= 1000", consistency_level="Strong"
    )
\end{lstlisting}

In addition to similarity and Boolean search, Milvus also supports online changes to data, different consistency levels for queries and queries on snapshots at different points in time. Most importantly, however, Milvus provides efficient \acrshort{nns} through state-of-the-art libraries and techniques such as FAISS \cite{Johnson:2019Billion}, ScANN \cite{Guo:2020Accelerating} and Spotify's Annoy \footnote{See https://github.com/spotify/annoy/, accessed in June 2022}, offers \acrshort{simd} and \acrshort{gpu} support and sports various indexes such as \acrshort{hnsw} \cite{Malkov:2018Efficient} or \acrshort{pq} \cite{Jegou:2010Product}. Dynamic data is supported by partitioning the data into segments and (re-)building of indexes on a segment per segment basis.