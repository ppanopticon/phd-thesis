\chapter{Multimedia Data Management}
\epiquote{Information is the oil of the 21st century, and analytics is the combustion engine.}{Peter Sondergaard, 2011}
\label{chapter:theory_multimedia_database}

The insight that special requirements must be addressed to accommodate multimedia data in traditional \acrshort{dbms} can be traced back to the 1990s \cite{Marcus:1996Foundations,Adjeroh:1997Multimedia} with many theoretical contributions but very little in terms of concrete (open-source) implementations. However, from a database research perspective, the topic received renewed attention with the publication of the \emph{Lowell Database Research Self-Assessment} in 2005 \cite{Abiteboul:2005Lowell} and the \emph{Claremont Report on Database Research} in 2008 \cite{Agrawal:2008Claremont}, both of which identified support for unstructured and multimedia data as well as the integration of information retrieval and database functionality as important research directions to consider.

In contrast, multimedia retrieval has traditionally relied on highly optimised data structures for storage and retrieval that were typically tightly coupled with the retrieval system itself in a monolithic architecture, offering little to no support for advanced data management functionality. It was not until the \emph{Ten Research Questions for Scalable Multimedia Analytics} \cite{Jonsson:2016Ten} that the issue of scalable data management for multimedia analytics and a convergence with database research was raised in the face of growing data collections \cite{Berns:2019V3C1,Rossetto:2021Insights}. Interestingly, many of the points raised by JÃ³nsson et al. are related to aspects listed in the 2014 \emph{The Beckman Report on Database Research} \cite{Abadi:2014Beckman}, namely the focus on ``end-to-end processing systems'' and ``data understanding'', demonstrating, that the two fields do have much in common.

However, while the amalgamation of text retrieval with classical database functionality became a topic of interest, multimedia retrieval remained a largely separated field of research from that of database research with the exception of a few isolated contributions that tried to converge specific aspects such as scoring and ranking \cite{Chengkai:2005RankSQL,Zhang:2006Boolean}, extensions to relational algebra based on fuzzy logic \cite{Montesi:1999Similarity}, or the conception of a multimedia query language \cite{Budikova:2012Query}.

\section{Early Multimedia Retrieval Systems}

As an alternative to a monolithic architecture, some of the early multimedia retrieval systems relied on and potentially combined existing database systems and built the special requirements for multimedia retrieval on top in either a single software component (2-tier architecture) or some type of middleware that then could be used by other systems (3-tier architecture). Examples that use the 2-tier approach include systems such as the video retrieval system OVID \cite{Oomoto:1993OVID}, the image retrieval system MARS \cite{Rui:1997Content}, or systems such as QBIC \cite{Flickner:1995Query} or MUVIS \cite{Kiranyaz:2003Muvis}. Unfortunately, neither of these systems are publicly available and therefore could not be examined in detail.

A prominent example is IBM's QBIC system \cite{Flickner:1995Query}, which allowed for (visual) similarity search on image and video data. The architecture of the QBIC system indicates the existence of a dedicated database layer used for storing features. However, there is little detail as to what type of database was used and what tasks were executed at a database level. Judging by the description, however, it seems that the queries were most likely processed by the ``matching engine'' and the database itself was only used for storage. It is also noteworthy that QBIC strictly distinguished between ``database population'' and ``database query'' as two different phases that took place independently of one another. A separation rather atypical for traditional database applications.

MUVIS \cite{Kiranyaz:2003Muvis} is another 2-tier multimedia retrieval system, which, in contrast to the aforementioned systems, combined multiple databases for storing image, video, and audio information separately without, however, providing an explicit reason for doing so. Again, there is little detail as to what tasks fell to these database layers and which tasks were carried out by the browsing layer built on top of it. Interestingly, we can again observe the distinction between ``indexing'' and ``retrieval'' as two separate and independent phases in the system's life cycle.

The \emph{Garlic Approach} \cite{Carey:1995Towards} proposed by IBM is an example of a 3-tier architecture. The Garlic middleware combines and integrates different, dedicated types of datastores to support heterogenous data under an extensible and modular query and runtime system. This system can then be used by other systems via standardised APIs. It was later used to integrate QBIC \cite{Cody:1995Querying}, which allowed for the combined search in visual and non-visual data. In a manner of speaking, Garlic can be seen as an early attempt at a polystore approach for multimedia data, like, for example, \emph{Polypheny-DB} \cite{Vogt:2018Polypheny}. 

The 2-tier architecture involving classical databases for information and multimedia retrieval is still very common today, with a tendency towards the use of NoSQL systems \cite{Muhleisen:2014Old,Oliveira:2017Performance}. Furthermore, there has been some work on retrofitting established \acrshort{dbms} for multimedia retrieval, either by using existing extension endpoints (e.g., user-defined data types or \glsentryfirstplural{udf}), by writing extensions, or altering core system components \cite{Guliato:2009PostgreSQL,Whang:2010Tightly,Giangreco:2014Adam,Fleites:2013Efficient,Whang:2015DB,Yang:2020Pase}.

\section{Libraries for Multimedia Retrieval}
\label{section:nns_libraries}

Instead of tightly integrating multimedia retrieval techniques with systems that use them, the past two decades bore witness to a trend towards open-source libraries that could be (re-)used. The standardised \acrshort{anns} benchmarking campaign known as \emph{ANN-Benchmarks} \cite{Aumueller:2017ANN},\footnote{See http://ann-benchmarks.com/; accessed July 2022.} for example, lists and evaluates 25 individual libraries, such as, \emph{Spotify Annoy},\footnote{See https://github.com/spotify/annoy; accessed July 2022.} ScaNN \cite{Guo:2020Accelerating} by Google Research, \emph{Hnswlib} \cite{Malkov:2018Efficient}, Facebook's \acrshort{faiss} \cite{Johnson:2019Billion}, or Yahoo's NGT \cite{Iwasaki2016:Pruned}.

A noteworthy early attempt at providing such a library for content-based image retrieval was \acrfull{lire} \cite{Luc:2008LIRE} -- an open-source Java library based on the Apache Lucene engine, which is typically used for fulltext search. \acrshort{lire} was later used and extended to create LIvRE \cite{Oliveira:2016Large}, a similar project but for video retrieval. While both \acrshort{lire} and LIvRE were an important step in the right direction, they unfortunately tightly integrated feature extraction and storage and were thus of limited use when trying to combine them with features that did not come pre-bundled. Nonetheless, these projects demonstrated Apache Lucene's capability of executing \acrshort{nns}. Consequently, Lucene based projects such as ElasticSearch\footnote{See https://www.elastic.co/; accessed July 2022.} or OpenSearch\footnote{See https://opensearch.org/; accessed July 2022.} still provide \acrshort{nns} and \acrshort{anns} through plugins and extensions such as Elastiknn.\footnote{See https://github.com/erikbern/ann-benchmarks/; accessed July 2022.}

An important step towards an engine for scalable \acrshort{nns} was the open-source library \acrfull{faiss} \cite{Johnson:2019Billion}. \acrshort{faiss} brings together different methods for \acrshort{anns} -- for example, \acrshort{pq} \cite{Jegou:2010Product}, \acrshort{hnsw} \cite{Malkov:2018Efficient}, and \acrshort{lsh} \cite{Indyk1998:Approximate} based index structures -- and provides  \acrshort{simd} and \acrshort{gpu} support both for indexing and querying. While \acrshort{faiss} is not a multimedia database system per-se, it provides a formidable foundation for building one.

However, even though all these projects are important and useful, they typically require integration into a system and do not provide an ``out-of-the-box'' solution that can be used via standardised interfaces such as \acrshort{sql}.

\section{Vector Database Engines}
\label{section:vector_databases}
The past few years have seen the conception of novel vector databases as either dedicated systems or extensions to existing ones. We refrain from presenting all these systems in detail and instead briefly name and describe a few, with the exception of one system that receives a more detailed discussion. Most of these systems have in common that they have been built as cloud-native applications with distribution in mind and that they are open-source. For every system, we reference the respective GitHub repository as well as the current version and the time of the last update to illustrate the activity in this particular domain.

\begin{description}
    \item[Weaviate (1.14.1, July 2022)]\footnote{See https://github.com/semi-technologies/weaviate/; accessed July 2022.} is a vector search engine that allows for the storage of vectors and attributes. Both the vector and attributes can be queried using the \emph{GraphQL} query language and it relies on \acrshort{hnsw} \cite{Malkov:2018Efficient} for \acrshort{anns}. Weaviate claims full \acrshort{crud} support, i.e., changes to entries are possible, which however limits the types of index structures used (currently, a custom \acrshort{hnsw} index is employed). The recent, major update to version 1.14.0 has added support for the Euclidean distance and the dot product.
    \item[Vald (1.5.5, July 2022)]\footnote{See https://github.com/vdaas/vald/; accessed July 2022.} is a vector search engine that relies on \acrshort{ngt} for \acrshort{anns}. It allows for similarity search as well as lookups by identifier using a gRPC based query API. A unique selling point is the \emph{asynchronous auto indexing} feature, which refreshes outdated indexes in the background in a non-blocking fashion, which allows for changes to the data.
    \item[Qdrant (0.8.5, July 2022)]\footnote{See https://github.com/qdrant/qdrant/; accessed July 2022.} is a similarity search engine that can be used to store high-dimensional vectors with additional attributes -- called payload. It can be characterised as a document database. Qdrant supports similarity search queries as well as filtering for payload attributes via either a gRPC or RESTful interface. Both payload attributes as well as vectors can be indexed and \acrshort{hnsw} \cite{Malkov:2018Efficient} is used for the latter. A query planner determines query execution in case multiple index structures are employed.
\end{description}

\subsection{Milvus}
\label{section:milvus}

Milvus (2.0.2, April 2022) \cite{Wang:2021Milvus} is a vector database system with its most recent 2.0 major release dating back to January 2022. Milvus is advertised as a ``vector database built for scalable similarity search'' and it relies on many of the libraries mentioned in \Cref{section:nns_libraries}. According to information on the official blog \footnote{See https://milvus.io/blog/; accessed July 2022.}, it was built as a cloud-native application with high scalability and availability in mind. It offers support for functionality that goes beyond mere \acrshort{nns}, such as, Boolean filtering as well as hybrid queries involving both similarity search and Boolean predicates on scalar data types as well as index support for both types of queries. In addition, Milvus is also capable of processing online changes to data (i.e., inserts and deletes).

Data in Milvus is organised into named collections, which can consist of multiple attributes and are distributed among multiple partitions. The logical data model and type system is similar to that of \cottontail{}, however, in addition to scalar types (e.g., \texttt{int}, \texttt{float}, \texttt{double} or \texttt{string}) it only allows for \texttt{float} or binary vectors. Milvus offers client libraries for Go, Java (and Kotlin), Python, and Node, which offers a user-friendly facade over the gRPC-based query interface and allows for data definition, data management, and querying.

Milvus provides fast \acrshort{nns} through a component named \emph{Knowwhere}, which provides a standardised operations interface that abstracts away the underlying state-of-the-art libraries and techniques such as FAISS \cite{Johnson:2019Billion}, Hnswlib \cite{Malkov:2018Efficient} and Spotify's Annoy \footnote{See https://github.com/spotify/annoy/; accessed in June 2022.} and which offers support for \acrshort{simd} and \acrshort{gpu} execution as well as various index structures. Dynamic data support for indexes is enabled by an eventual consistency model, wherein data is being (re-)indexed on a partition by partition basis, i.e., if changes occur, only the affected partitions must be rebuilt. This mechanism is combined with a \acrshort{mvcc}-based snapshot isolation, which also allows for querying the state of the database at a specific point in time. Deletes are filtered on the fly during query time based on \emph{tombstone} markers in the database, a concept known from Apache Cassandra.

It is important to mention that while data held by Milvus resides on disk, a collection must be loaded into main memory in its entirety in order to be able to query it. Therefore, for Milvus to be able to execute \acrshort{nns} or Boolean filtering, all the data (or at least the indexes), must fit into the combined main memory of the available query nodes.

\section{Databases for Data Science}
With the start of the NoSQL movement some researchers began to address the shortcomings of the relational data model in the context of scientific applications and proposed different architectures, such as array databases. Early examples include SimDB \cite{Silva:2010SimDB} or SciDB \cite{Stonebraker:2011Architecture,Stonebraker:2013SciDB}. The topic of science-oriented databases has been listed as highly relevant in the 2020 database research self-assessment report, due to the continued trend towards data driven applications, data science, and data analytics \cite{Abadi:2020Seattle}. In addition to challenges in the domain of data governance, one focus that was identified is the convergence between linear algebra and relational algebra in an efficient and effective framework \cite{Luo:2018Scalable}, an effort that could also greatly benefit multimedia retrieval and analytics applications \cite{Berry:1995Using}. Another area of interest is the use of specialised hardware, such as \acrshort{fpga}s or \acrshort{gpu}s, to accelerate query evaluation.
 
\section{vitrivr, ADAM and \texorpdfstring{\adampro{}}{ADAMpro}}
\label{section:vitrivr_adam_adampro}
To the best of our knowledge, the open-source \vitrivr{} video retrieval \cite{Rossetto:2016Vitrivr} and later multimedia retrieval \cite{Gasser:2019Multimodal} system was one of the first systems to advertise a 3-tier architecture that included a specialised multimedia database in addition to a dedicated feature extraction and score fusion engine as well as a dedicated user interface. The first-generation database layer came in the form of ADAM \cite{Giangreco:2014Adam}, which can be seen as an early attempt at bridging the gap between databases and multimedia retrieval. At its core, ADAM was an extension to PostgreSQL -- called \emph{dbADAM} -- that added support for \acrshort{nns} and \acrshort{vaf}-based indexes \cite{Weber:1998Va}. In that respect it is comparable to projects like \emph{pgvector},\footnote{See https://github.com/pgvector/pgvector/; accessed July 2022.} \emph{pgANN},\footnote{See https://github.com/netrasys/pgANN/; accessed July 2022.} and PASE \cite{Yang:2020Pase}, which however use more recent index structures such as \acrshort{pq} \cite{Jegou:2010Product} and \acrshort{hnsw} \cite{Malkov:2018Efficient}. The \emph{dbADAM} component was complemented by \emph{wsADAM}, which could orchestrate query execution over multiple PostgreSQL instances using MapReduce \cite{Dean:2008Mapreduce}. ADAM's successor \adampro{} \cite{Giangreco:2016Adam} tried to address performance limitations with a multi-model (polyglot) database system approach that allowed for query execution on different storage engines and distribution of data and computation using \emph{Apache Spark}, a large-scale data analytics platform. \adampro{} still relied on PostgreSQL for storage of relational data but used \emph{Apache Parquet} to store and access features. 

The work on \adampro{} was complemented by a theoretical model \cite{Giangreco:2018Database}: Giangreco et al. demonstrated that for a database system to be able to support similarity search given the relational model as described in \cref{section:relational_data_model}, one can extend the set system of allowed data domains $\mathbb{D}$ by $\domain \subset \symreal^{dim}, dim \in \symnatural_{>1}$ and postulate the existence of a relational similarity operator $\tau_{\delta(\cdot,\cdot),a,q}(\relation)$ that can be used to express similarity search. According to the definition, such an operation introduces an implicit attribute in the underlying relation $\relation$, which in turn induces an ascending ordering of the tuples. Two variants of $\tau$ were proposed, namely $\tau^{kNN}_{\delta(\cdot,\cdot),a,q}$ (\acrshort{nns}) and $\tau^{\epsilon NN}_{\delta(\cdot,\cdot),a,q}$ (range search), which select tuples by their cardinality $k$ or a maximum cut-off distance $\epsilon$ respectively.

While postulating two new relational operators is well within the tradition of how relational algebra has been extended over the years (see \Cref{section:rel_extensions}), it comes with certain limitations. In its proposed form, $\tau$ addresses several functions at once: It
\begin{enumerate*}[label=(\roman*)]
    \item specifies the distance function that should be evaluated,
    \item generates an implicit distance attribute on the underlying relation $\relation$,
    \item imposes an ascending ordering and limits the cardinality of $\relation$ based on a predicate or a specified limit.
\end{enumerate*}

Even though it may be straightforward to implement, the amalgamation of all this functionality into a single operation is very specifically tailored to the use case of \acrshort{nns} and range search and only of limited value when considering more general distance-based query operations. If one were to, for example, obtain the $k$ farthest neighbours rather than the $k$ nearest neighbours, as necessary when doing \acrshort{mips} or obtaining negative examples, they would have to either change the distance function, define new operators, or extend the definition of $\tau$. 

Another important issue with the definition of $\tau$ in its proposed form is that despite the two operations $\tau^{kNN}_{\delta(\cdot,\cdot),a,q}$ and $\tau^{\epsilon NN}_{\delta(\cdot,\cdot),a,q}$ serving a very similar purpose and sharing the core definition, they behave very differently with respect to other operations. Generally, $\tau^{kNN}_{\delta(\cdot,\cdot),a,q}(\relation)$ does not commute with any selection $\sigma$ due to the inherent limiting of the cardinality to a constant value, hence:

\begin{equation}
    \label{equation:commutation_of_tau}
    \sigma(\tau^{kNN}_{\delta(\cdot,\cdot),a,q}(\relation)) \neq \tau^{kNN}_{\delta(\cdot,\cdot),a,q}(\sigma(\relation))
\end{equation}

The left side of \cref{equation:commutation_of_tau} filters the results of a kNN-search on $\relation$, thus returning $n \leq k$ results, wherein $n = k$ only if $\sigma$ matches all tuples. The right side of \cref{equation:commutation_of_tau} performs a kNN-search on a pre-filtered relation $\relation$, also returning $n \leq k$ entries. However, $n$ will only be smaller than $k$ if $\sigma$ selects fewer than $k$ tuples.